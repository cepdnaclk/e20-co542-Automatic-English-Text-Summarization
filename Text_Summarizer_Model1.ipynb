{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Load Subset of the Dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")\n",
        "\n",
        "\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"highlights\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in labels[\"input_ids\"][i]]\n",
        "        for i in range(len(labels[\"input_ids\"]))\n",
        "    ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "UFYdMkD-hjjD",
        "outputId": "4d3bcb86-e06a-4a92-c28f-668a42ceaaeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-5-b95b1ec876fc>:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1077' max='1077' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1077/1077 25:14, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.787000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.651600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.597700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.615600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.559500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.580700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.572400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.564400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.539200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.508400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1077, training_loss=1.5937534002872562, metrics={'train_runtime': 1515.6891, 'train_samples_per_second': 5.683, 'train_steps_per_second': 0.711, 'total_flos': 5244954311393280.0, 'train_loss': 1.5937534002872562, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"The U.S. economy added 850,000 jobs in June, a sign of continued recovery as businesses reopen and consumers spend more. The unemployment rate, however, ticked up slightly to 5.9% from 5.8% in May.\",\n",
        "        \"A massive wildfire in northern California has scorched over 150,000 acres, forcing thousands to evacuate. Firefighters are struggling to contain the blaze amid high temperatures and strong winds.\",\n",
        "        \"Scientists have discovered a new species of dinosaur in Argentina. The creature, named 'Llukalkan aliocranianus,' lived approximately 80 million years ago and is believed to have been a formidable predator.\",\n",
        "        \"The Tokyo 2020 Olympics, postponed due to the COVID-19 pandemic, are set to begin with strict health protocols in place. Athletes will undergo regular testing, and spectators will be limited to local residents.\",\n",
        "        \"A recent study suggests that drinking coffee may reduce the risk of developing Alzheimer's disease. Researchers found that participants who consumed higher amounts of caffeine had a lower incidence of the neurodegenerative condition.\",\n",
        "        \"The United Nations has called for an immediate ceasefire in the ongoing conflict in Yemen. The humanitarian crisis has worsened, with millions facing famine and limited access to medical supplies.\",\n",
        "        \"Tech giant Apple has announced plans to invest $1 billion in building a new campus in North Carolina. The facility is expected to create thousands of jobs and bolster the state's economy.\"]\n",
        "\n",
        "for text in texts:\n",
        "  input_ids = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\").to(model.device)\n",
        "  output = model.generate(\n",
        "      input_ids,\n",
        "      max_length=100,\n",
        "      num_beams=3,\n",
        "      early_stopping=True,\n",
        "      do_sample=True,\n",
        "      temperature=0.9,\n",
        "      top_k=50,\n",
        "      top_p=0.95\n",
        "  )\n",
        "  summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI3ApSvhozbl",
        "outputId": "96f652d5-94d0-4b5f-823f-958fd2519e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U.S. economy added 850,000 jobs in June, sign of continued recovery . The unemployment rate ticked up slightly to 5.9% from 5.8% .\n",
            "A massive wildfire in northern California has scorched over 150,000 acres . Firefighters are struggling to contain the blaze amid high temperatures and strong winds .\n",
            "Scientists have discovered a new species of dinosaur in Argentina . The creature, named 'Lukalkan aliocranianus', lived approximately 80 million years ago .\n",
            "Athletes will undergo regular testing, and spectators will be limited to local residents . The Tokyo 2020 Olympics are set to begin with strict health protocols in place .\n",
            "Researchers found people who consumed more caffeine had a lower incidence of Alzheimer's disease .\n",
            "The humanitarian crisis has worsened, with millions facing famine .\n",
            "Tech giant Apple has announced plans to invest $1 billion in a new campus . The facility is expected to create thousands of jobs .\n"
          ]
        }
      ]
    }
  ]
}