{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from bert_score import score\n",
    "from summac.model_summac import SummaCConv\n",
    "\n",
    "# Load metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "bleu = load_metric(\"bleu\")\n",
    "meteor = load_metric(\"meteor\")\n",
    "summac_model = SummaCConv(model_name=\"vitc\", granularity=\"sentence\")\n",
    "\n",
    "\n",
    "def evaluate_summarization(model, dataset, num_samples=10):\n",
    "    predictions, references = [], []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        article = dataset[\"article\"][i]\n",
    "        reference_summary = dataset[\"highlights\"][i]\n",
    "\n",
    "        inputs = tokenizer(\"summarize: \" + article, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        output_ids = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "        predicted_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append(predicted_summary)\n",
    "        references.append(reference_summary)\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bleu_scores = bleu.compute(\n",
    "      predictions=[pred.split() for pred in predictions],\n",
    "      references=[[ref.split()] for ref in references]\n",
    "    )\n",
    "    meteor_scores = meteor.compute(predictions=predictions, references=references)\n",
    "    # bert_F1_score - > mean between bert_precision and bert_recall\n",
    "    bert_precision, bert_recall, bert_F1_score = score(predictions, references, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_score = bert_F1_score.mean().item()\n",
    "    summac_scores = summac_model.score(references, predictions)[\"scores\"]\n",
    "    return rouge_scores, bleu_scores, meteor_scores, bert_score, summac_scores\n",
    "\n",
    "# Evaluate on validation set\n",
    "rouge_scores, bleu_scores, meteor_scores, bert_score, summac_scores = evaluate_summarization(model, dataset[\"validation\"])\n",
    "print(rouge_scores)\n",
    "print(bleu_scores)\n",
    "print(meteor_scores)\n",
    "print(bert_score)\n",
    "print(summac_scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
